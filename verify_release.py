#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
verify_release.py
ç¤ºä¾‹ï¼šåŸºäºæ¨¡æ¿çš„ GitHub å‘å¸ƒæµç¨‹éªŒè¯è„šæœ¬
ä¾èµ–ï¼šrequests, python-dotenv
pip install -r requirements.txt
"""
import os
import sys
import requests
import base64
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv
from datetime import datetime, timezone

# --------------------------
# å…·ä½“é…ç½®ï¼ˆç¤ºä¾‹ï¼Œå¯é€šè¿‡æ›¿æ¢æ­¤ dict æˆ–å°†å…¶åŠ è½½ä¸ºå¤–éƒ¨æ–‡ä»¶è¿›è¡Œè¦†ç›–ï¼‰
# --------------------------
CONFIG: Dict = {
    "ENV_CONFIG": {
        "env_file_name": ".mcp_env",
        "github_token_var": "MCP_GITHUB_TOKEN",
        "github_org_var": "GITHUB_EVAL_ORG",
    },
    "GITHUB_API_CONFIG": {
        "api_accept_format": "application/vnd.github.v3+json",
        "success_status_code": 200,
        "not_found_status_code": 404,
        "api_per_page": 100,
        "file_encoding": "utf-8",
        "merge_commit_parent_count": 1,
    },
    "VERIFICATION_FLOW_CONFIG": {
        "step_number_format": {
            "env_check": "1/6",
            "branch_check": "2/6",
            "file_check": "3/6",
            "pr_search": "4/6",
            "pr_merge_target": "5/6",
            "merge_method": "6/6",
        },
        "separator_length": 60,
        "success_message": "ğŸ‰ å‘å¸ƒæµç¨‹æ‰€æœ‰éªŒè¯æ­¥éª¤é€šè¿‡ï¼",
        "exit_code": {"success": 0, "failure": 1},
        # report filename placed at repository path ./mcpmark/release_verification/
        "report_file": "verification_report.txt",
    },
    "TARGET_RESOURCE_CONFIG": {
        "target_repo": "my-project",  # è¯·æ”¹ä¸ºå®é™…ç›®æ ‡ä»“åº“å
        "branches": {"release_branch": "main", "base_branch": "main"},
    },
    "FILE_VERIFICATION_CONFIG": {
        "required_files": [
            {
                "name": "ç¼–ç é…ç½®æ–‡ä»¶",
                "path": "src/encoding.rs",
                "branch": "main",
                "required_content": 'FormattingToken::MetaSep => "<|meta_sep|>"',
                "min_size": 500,
            },
            {
                "name": "æ³¨å†Œè¡¨æ–‡ä»¶",
                "path": "src/registry.rs",
                "branch": "main",
                "required_contents": [
                    '(FormattingToken::MetaSep, "<|meta_sep|>")',
                    '(FormattingToken::MetaEnd, "<|meta_end|>")',
                ],
                "min_size": 500,
            },
            {
                "name": "ç‰ˆæœ¬é…ç½®æ–‡ä»¶",
                "path": "Cargo.toml",
                "branch": "main",
                "required_content": 'version = "1.1.0"',
                "min_size": 200,
            },
            {
                "name": "å˜æ›´æ—¥å¿—",
                "path": "CHANGELOG.md",
                "branch": "main",
                "required_keywords": [
                    "## [1.1.0] - 2025-08-07",
                    "MetaSep token mapping bug",
                    "Fixed MetaSep token",
                ],
                "min_size": 300,
            },
        ]
    },
    "PR_VERIFICATION_CONFIG": {
        "pr_title_keyword": "Release v1.1.0",
        "pr_state": "closed",
        "required_merge_method": "Squash and Merge",
    },
    # output directory relative to repository root where report will be written
    "OUTPUT_DIR": "mcpmark/release_verification"
}


# --------------------------
# å°å·¥å…·å‡½æ•°
# --------------------------
def _load_env() -> Tuple[Optional[str], Optional[str]]:
    env_file = CONFIG["ENV_CONFIG"]["env_file_name"]
    # å¦‚æœå­˜åœ¨ env æ–‡ä»¶å°±åŠ è½½å®ƒï¼ˆå…è®¸åœ¨ CI ä¸­é€šè¿‡ç¯å¢ƒå˜é‡æ³¨å…¥ï¼‰
    if os.path.exists(env_file):
        load_dotenv(env_file)
    github_token = os.environ.get(CONFIG["ENV_CONFIG"]["github_token_var"])
    github_org = os.environ.get(CONFIG["ENV_CONFIG"]["github_org_var"])
    return github_token, github_org


def _build_headers(github_token: str) -> Dict[str, str]:
    return {
        "Authorization": f"token {github_token}",
        "Accept": CONFIG["GITHUB_API_CONFIG"]["api_accept_format"],
        "User-Agent": "GitHub-Release-Verification-Tool",
    }


def _call_github_api(endpoint: str, headers: Dict[str, str], org: str, repo: str, params: Optional[Dict] = None) -> Tuple[bool, Optional[Dict]]:
    url = f"https://api.github.com/repos/{org}/{repo}/{endpoint}"
    try:
        r = requests.get(url, headers=headers, timeout=15, params=params)
        if r.status_code == CONFIG["GITHUB_API_CONFIG"]["success_status_code"]:
            return True, r.json()
        elif r.status_code == CONFIG["GITHUB_API_CONFIG"]["not_found_status_code"]:
            print(f"[API æç¤º] èµ„æº {endpoint} æœªæ‰¾åˆ°ï¼ˆ404ï¼‰", file=sys.stderr)
            return False, None
        else:
            print(f"[API é”™è¯¯] {endpoint} è¿”å›çŠ¶æ€ {r.status_code}ï¼š{r.text[:300]}", file=sys.stderr)
            return False, None
    except Exception as e:
        print(f"[API å¼‚å¸¸] è°ƒç”¨ {endpoint} å‡ºé”™ï¼š{e}", file=sys.stderr)
        return False, None


def _check_branch_exists(branch_name: str, headers: Dict[str, str], org: str, repo: str) -> bool:
    success, _ = _call_github_api(f"branches/{branch_name}", headers, org, repo)
    return success


def _get_file_content(file_path: str, branch: str, headers: Dict[str, str], org: str, repo: str) -> Optional[str]:
    success, data = _call_github_api(f"contents/{file_path}?ref={branch}", headers, org, repo)
    if not success or not data:
        return None
    try:
        content_b64 = data.get("content", "").replace("\n", "")
        return base64.b64decode(content_b64).decode(CONFIG["GITHUB_API_CONFIG"]["file_encoding"], errors="replace")
    except Exception as e:
        print(f"[æ–‡ä»¶è§£ç é”™è¯¯] {file_path}: {e}", file=sys.stderr)
        return None


def _find_merged_pr(pr_title_keyword: str, base_branch: str, pr_state: str, headers: Dict[str, str], org: str, repo: str) -> Optional[Dict]:
    # Try first page(s) looking for matching merged PR
    page = 1
    per_page = CONFIG["GITHUB_API_CONFIG"]["api_per_page"]
    while True:
        endpoint = f"pulls"
        params = {"state": pr_state, "base": base_branch, "per_page": per_page, "page": page}
        success, pr_list = _call_github_api(f"{endpoint}", headers, org, repo, params=params)
        if not success or not pr_list:
            return None
        if not isinstance(pr_list, list):
            return None
        for pr in pr_list:
            title = pr.get("title", "")
            merged_at = pr.get("merged_at")
            if pr_title_keyword.lower() in title.lower() and merged_at:
                return pr
        # Stop if fewer than per_page results returned
        if len(pr_list) < per_page:
            break
        page += 1
    return None


def _verify_squash_merge(pr_number: int, headers: Dict[str, str], org: str, repo: str) -> Tuple[str, int]:
    """
    Return tuple: (method_str, parent_count)
    method_str: "Squash and Merge" | "OTHER" | "not found"
    parent_count: integer number of parents (0 if not found)
    """
    success, pr_detail = _call_github_api(f"pulls/{pr_number}", headers, org, repo)
    if not success or not pr_detail:
        return "not found", 0
    merge_commit_sha = pr_detail.get("merge_commit_sha")
    if not merge_commit_sha:
        return "not found", 0
    success, commit_detail = _call_github_api(f"commits/{merge_commit_sha}", headers, org, repo)
    if not success or not commit_detail:
        return "not found", 0
    parent_count = len(commit_detail.get("parents", []))
    commit_msg = commit_detail.get("commit", {}).get("message", "")
    expected_parent_count = CONFIG["GITHUB_API_CONFIG"]["merge_commit_parent_count"]
    if parent_count == expected_parent_count and f"#{pr_number}" in commit_msg:
        return "Squash and Merge", parent_count
    return "OTHER", parent_count


# --------------------------
# éªŒè¯æ­¥éª¤å®ç°
# --------------------------
def _verify_environment() -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['env_check']} éªŒè¯ç¯å¢ƒé…ç½®...")
    token, org = _load_env()
    if not token:
        print(f"[ç¯å¢ƒé”™è¯¯] æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ {CONFIG['ENV_CONFIG']['github_token_var']}", file=sys.stderr)
        return None, None, None
    if not org:
        print(f"[ç¯å¢ƒé”™è¯¯] æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ {CONFIG['ENV_CONFIG']['github_org_var']}", file=sys.stderr)
        return None, None, None
    headers = _build_headers(token)
    print(f"[ç¯å¢ƒå°±ç»ª] org={org}, token=å·²é…ç½®")
    return token, org, headers


def _verify_branches(headers: Dict[str, str], org: str, repo: str) -> bool:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['branch_check']} éªŒè¯åˆ†æ”¯å­˜åœ¨æ€§...")
    b = CONFIG["TARGET_RESOURCE_CONFIG"]["branches"]
    if not _check_branch_exists(b["release_branch"], headers, org, repo):
        print(f"[åˆ†æ”¯é”™è¯¯] å‘å¸ƒåˆ†æ”¯ {b['release_branch']} ä¸å­˜åœ¨", file=sys.stderr)
        return False
    if not _check_branch_exists(b["base_branch"], headers, org, repo):
        print(f"[åˆ†æ”¯é”™è¯¯] åŸºç¡€åˆ†æ”¯ {b['base_branch']} ä¸å­˜åœ¨", file=sys.stderr)
        return False
    print(f"[åˆ†æ”¯éªŒè¯é€šè¿‡] release={b['release_branch']}, base={b['base_branch']}")
    return True


def _verify_required_files(headers: Dict[str, str], org: str, repo: str) -> Tuple[bool, List[str]]:
    files = CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"]
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['file_check']} éªŒè¯å…³é”®æ–‡ä»¶ï¼ˆ{len(files)}ï¼‰...")
    ok = True
    failed_paths: List[str] = []
    for f in files:
        print(f"\n  éªŒè¯ {f['name']} -> {f['path']} @ {f['branch']}")
        # Ignore system files explicitly (though target files are named)
        if f["path"] in [".DS_Store", "Thumbs.db"]:
            print(f"  [å¿½ç•¥] ç³»ç»Ÿæ–‡ä»¶ {f['path']}")
            continue
        content = _get_file_content(f["path"], f["branch"], headers, org, repo)
        if not content:
            print(f"  [æ–‡ä»¶é”™è¯¯] æ— æ³•è¯»å– {f['path']}", file=sys.stderr)
            ok = False
            failed_paths.append(f['path'])
            continue
        if len(content) < f["min_size"]:
            print(f"  [æ–‡ä»¶é”™è¯¯] {f['path']} å¤§å° {len(content)} < {f['min_size']}", file=sys.stderr)
            ok = False
            failed_paths.append(f['path'])
            continue
        # å†…å®¹æ ¡éªŒï¼ˆå•ã€å¤šã€å…³é”®è¯ï¼‰
        if "required_content" in f:
            if f["required_content"] not in content:
                print(f"  [å†…å®¹é”™è¯¯] ç¼ºå°‘: {f['required_content'][:80]}...", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        if "required_contents" in f:
            missing = [c for c in f["required_contents"] if c not in content]
            if missing:
                print(f"  [å†…å®¹é”™è¯¯] ç¼ºå¤±å¤šé¡¹: {missing}", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        if "required_keywords" in f:
            missing = [k for k in f["required_keywords"] if k not in content]
            if missing:
                print(f"  [å†…å®¹é”™è¯¯] ç¼ºå¤±å…³é”®è¯: {missing}", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        print(f"  [æ–‡ä»¶éªŒè¯é€šè¿‡] {f['path']}")
    return ok, failed_paths


def _verify_release_pr(headers: Dict[str, str], org: str, repo: str) -> Optional[Dict]:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['pr_search']} æŸ¥æ‰¾å‘å¸ƒ PR...")
    pr_cfg = CONFIG["PR_VERIFICATION_CONFIG"]
    base = CONFIG["TARGET_RESOURCE_CONFIG"]["branches"]["base_branch"]
    pr = _find_merged_pr(pr_cfg["pr_title_keyword"], base, pr_cfg["pr_state"], headers, org, repo)
    if not pr:
        print(f"[PR é”™è¯¯] æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å·²åˆå¹¶ PRï¼ˆåŒ…å« '{pr_cfg['pr_title_keyword']}'ï¼‰", file=sys.stderr)
        return None
    print(f"[PR æ‰¾åˆ°] #{pr.get('number')} - {pr.get('title')}")
    return pr


# --------------------------
# æŠ¥å‘Šç”Ÿæˆ
# --------------------------
def _write_report(report_dir: str, payload_lines: List[str]) -> bool:
    # ensure dir exists
    os.makedirs(report_dir, exist_ok=True)
    report_file = os.path.join(report_dir, CONFIG["VERIFICATION_FLOW_CONFIG"]["report_file"])
    try:
        with open(report_file, "w", encoding="utf-8", newline="\n") as f:
            f.write("\n".join(payload_lines))
        print(f"[æŠ¥å‘Šå·²å†™å…¥] {report_file}")
        return True
    except Exception as e:
        print(f"[æŠ¥å‘Šå†™å…¥é”™è¯¯] {e}", file=sys.stderr)
        return False


def run_verification(cfg: Dict) -> bool:
    global CONFIG
    CONFIG = cfg
    sep = "=" * CONFIG["VERIFICATION_FLOW_CONFIG"]["separator_length"]
    print(sep)
    print("å¼€å§‹æ‰§è¡Œ GitHub å‘å¸ƒæµç¨‹éªŒè¯ï¼ˆç¤ºä¾‹ï¼‰")
    print(sep)

    _, org, headers = _verify_environment()
    if not org or not headers:
        # prepare fail report
        repo = CONFIG["TARGET_RESOURCE_CONFIG"]["target_repo"]
        lines = [
            "verification result: FAIL",
            f"repository: {org or 'UNKNOWN'}/{repo}",
            f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
            "release PR: not found",
            "merge method: not found  parents: 0",
            "files checked: 0/0  failed files: none",
            f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
        ]
        out_dir = CONFIG.get("OUTPUT_DIR", "")
        if out_dir:
            _write_report(out_dir, lines)
        return False

    repo = CONFIG["TARGET_RESOURCE_CONFIG"]["target_repo"]

    # Step 2: branches
    if not _verify_branches(headers, org, repo):
        result = "FAIL"
        pr_line = "release PR: not found"
        merge_line = "merge method: not found  parents: 0"
        files_checked_line = "files checked: 0/0  failed files: none"
        lines = [
            f"verification result: {result}",
            f"repository: {org}/{repo}",
            f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
            pr_line,
            merge_line,
            files_checked_line,
            f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
        ]
        _write_report(CONFIG.get("OUTPUT_DIR", ""), lines)
        return False

    # Step 3: files
    files_ok, failed_files = _verify_required_files(headers, org, repo)

    # Step 4: find PR
    pr = _verify_release_pr(headers, org, repo)
    pr_line = "release PR: not found"
    merge_line = "merge method: not found  parents: 0"
    pr_number = None
    pr_title = ""
    method_str = "not found"
    parents = 0
    if pr:
        pr_number = pr.get("number")
        pr_title = pr.get("title", "").replace("\n", " ")
        pr_line = f"release PR: #{pr_number}  title: {pr_title}"
        # Step 5 & 6: merge target and method
        # Verify merge target
        success, pr_detail = _call_github_api(f"pulls/{pr_number}", headers, org, repo)
        if success and pr_detail:
            actual_base = pr_detail.get("base", {}).get("ref")
            expected_base = CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']
            # if target mismatch, set fail later
            if actual_base != expected_base:
                print(f"[åˆå¹¶ç›®æ ‡é”™è¯¯] PR #{pr_number} åˆå¹¶åˆ°ã€Œ{actual_base}ã€ï¼Œé¢„æœŸã€Œ{expected_base}ã€", file=sys.stderr)
        # verify merge method
        method_str, parents = _verify_squash_merge(pr_number, headers, org, repo)
        merge_line = f"merge method: {method_str}  parents: {parents}"
    else:
        # pr not found -> failure
        pass

    # decide overall PASS/FAIL
    passed_count = sum(1 for _ in CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"]) - len(failed_files)
    total_required = len(CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"])
    files_checked_line = f"files checked: {total_required - len(failed_files)}/{total_required}  failed files: {','.join(failed_files) if failed_files else 'none'}"

    overall_ok = all([
        files_ok,
        pr is not None,
        (pr is None or (method_str == "Squash and Merge")),
    ])

    result = "PASS" if overall_ok else "FAIL"

    lines = [
        f"verification result: {result}",
        f"repository: {org}/{repo}",
        f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
        pr_line,
        merge_line,
        files_checked_line,
        f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
    ]

    out_dir = CONFIG.get("OUTPUT_DIR", "")
    if out_dir:
        _write_report(out_dir, lines)

    # ---- æ–°å¢ï¼šå½“éªŒè¯é€šè¿‡æ—¶åœ¨æ§åˆ¶å°æ‰“å°éªŒè¯ä¿¡æ¯ï¼ˆreport å†…å®¹ï¼‰ ----
    if overall_ok:
        print("\n" + sep)
        # success message from config (if any)
        success_msg = CONFIG['VERIFICATION_FLOW_CONFIG'].get('success_message')
        if success_msg:
            print(success_msg)
        # print the report lines to stdout
        for line in lines:
            print(line)
        print(sep + "\n")

    return overall_ok


if __name__ == "__main__":
    # è¿è¡Œã€‚è‹¥è¦å¤–éƒ¨è‡ªå®šä¹‰é…ç½®ï¼Œå¯åœ¨æ­¤å¤„åŠ è½½ JSON å¹¶ä¼ å…¥ run_verification
    ok = run_verification(CONFIG)
    sys.exit(CONFIG["VERIFICATION_FLOW_CONFIG"]["exit_code"]["success"] if ok else CONFIG["VERIFICATION_FLOW_CONFIG"]["exit_code"]["failure"])
