#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
verify_release.py
Á§∫‰æãÔºöÂü∫‰∫éÊ®°ÊùøÁöÑ GitHub ÂèëÂ∏ÉÊµÅÁ®ãÈ™åËØÅËÑöÊú¨
‰æùËµñÔºörequests, python-dotenv
pip install -r requirements.txt
"""
import os
import sys
import requests
import base64
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv
from datetime import datetime, timezone

# --------------------------
# ÂÖ∑‰ΩìÈÖçÁΩÆÔºàÁ§∫‰æãÔºåÂèØÈÄöËøáÊõøÊç¢Ê≠§ dict ÊàñÂ∞ÜÂÖ∂Âä†ËΩΩ‰∏∫Â§ñÈÉ®Êñá‰ª∂ËøõË°åË¶ÜÁõñÔºâ
# --------------------------
CONFIG: Dict = {
    "ENV_CONFIG": {
        "env_file_name": ".mcp_env",
        "github_token_var": "MCP_GITHUB_TOKEN",
        "github_org_var": "GITHUB_EVAL_ORG",
    },
    "GITHUB_API_CONFIG": {
        "api_accept_format": "application/vnd.github.v3+json",
        "success_status_code": 200,
        "not_found_status_code": 404,
        "api_per_page": 100,
        "file_encoding": "utf-8",
        "merge_commit_parent_count": 1,
    },
    "VERIFICATION_FLOW_CONFIG": {
        "step_number_format": {
            "env_check": "1/6",
            "branch_check": "2/6",
            "file_check": "3/6",
            "pr_search": "4/6",
            "pr_merge_target": "5/6",
            "merge_method": "6/6",
        },
        "separator_length": 60,
        "success_message": "üéâ ÂèëÂ∏ÉÊµÅÁ®ãÊâÄÊúâÈ™åËØÅÊ≠•È™§ÈÄöËøáÔºÅ",
        "exit_code": {"success": 0, "failure": 1},
        # report filename placed at repository path ./mcpmark/release_verification/
        "report_file": "verification_report.txt",
    },
    "TARGET_RESOURCE_CONFIG": {
        "target_repo": "harmony",  # ËØ∑Êîπ‰∏∫ÂÆûÈôÖÁõÆÊ†á‰ªìÂ∫ìÂêç
        "branches": {"release_branch": "release-v1.1.0", "base_branch": "main"},
    },
    "FILE_VERIFICATION_CONFIG": {
        "required_files": [
            {
                "name": "ÁºñÁ†ÅÈÖçÁΩÆÊñá‰ª∂",
                "path": "src/encoding.rs",
                "branch": "main",
                "required_content": 'FormattingToken::MetaSep => "<|meta_sep|>"',
                "min_size": 500,
            },
            {
                "name": "Ê≥®ÂÜåË°®Êñá‰ª∂",
                "path": "src/registry.rs",
                "branch": "main",
                "required_contents": [
                    '(FormattingToken::MetaSep, "<|meta_sep|>")',
                    '(FormattingToken::MetaEnd, "<|meta_end|>")',
                ],
                "min_size": 500,
            },
            {
                "name": "ÁâàÊú¨ÈÖçÁΩÆÊñá‰ª∂",
                "path": "Cargo.toml",
                "branch": "main",
                "required_content": 'version = "1.1.0"',
                "min_size": 200,
            },
            {
                "name": "ÂèòÊõ¥Êó•Âøó",
                "path": "CHANGELOG.md",
                "branch": "main",
                "required_keywords": [
                    "## [1.1.0] - 2025-08-07",
                    "MetaSep token mapping bug",
                    "Fixed MetaSep token",
                ],
                "min_size": 300,
            },
        ]
    },
    "PR_VERIFICATION_CONFIG": {
        "pr_title_keyword": "Release v1.1.0",
        "pr_state": "closed",
        "required_merge_method": "Squash and Merge",
    },
    # output directory relative to repository root where report will be written
    "OUTPUT_DIR": "mcpmark/release_verification"
}


# --------------------------
# Â∞èÂ∑•ÂÖ∑ÂáΩÊï∞
# --------------------------
def _load_env() -> Tuple[Optional[str], Optional[str]]:
    env_file = CONFIG["ENV_CONFIG"]["env_file_name"]
    # Â¶ÇÊûúÂ≠òÂú® env Êñá‰ª∂Â∞±Âä†ËΩΩÂÆÉÔºàÂÖÅËÆ∏Âú® CI ‰∏≠ÈÄöËøáÁéØÂ¢ÉÂèòÈáèÊ≥®ÂÖ•Ôºâ
    if os.path.exists(env_file):
        load_dotenv(env_file)
    github_token = os.environ.get(CONFIG["ENV_CONFIG"]["github_token_var"])
    github_org = os.environ.get(CONFIG["ENV_CONFIG"]["github_org_var"])
    return github_token, github_org


def _build_headers(github_token: str) -> Dict[str, str]:
    return {
        "Authorization": f"token {github_token}",
        "Accept": CONFIG["GITHUB_API_CONFIG"]["api_accept_format"],
        "User-Agent": "GitHub-Release-Verification-Tool",
    }


def _call_github_api(endpoint: str, headers: Dict[str, str], org: str, repo: str, params: Optional[Dict] = None) -> Tuple[bool, Optional[Dict]]:
    url = f"https://api.github.com/repos/{org}/{repo}/{endpoint}"
    try:
        r = requests.get(url, headers=headers, timeout=15, params=params)
        if r.status_code == CONFIG["GITHUB_API_CONFIG"]["success_status_code"]:
            return True, r.json()
        elif r.status_code == CONFIG["GITHUB_API_CONFIG"]["not_found_status_code"]:
            print(f"[API ÊèêÁ§∫] ËµÑÊ∫ê {endpoint} Êú™ÊâæÂà∞Ôºà404Ôºâ", file=sys.stderr)
            return False, None
        else:
            print(f"[API ÈîôËØØ] {endpoint} ËøîÂõûÁä∂ÊÄÅ {r.status_code}Ôºö{r.text[:300]}", file=sys.stderr)
            return False, None
    except Exception as e:
        print(f"[API ÂºÇÂ∏∏] Ë∞ÉÁî® {endpoint} Âá∫ÈîôÔºö{e}", file=sys.stderr)
        return False, None


def _check_branch_exists(branch_name: str, headers: Dict[str, str], org: str, repo: str) -> bool:
    success, _ = _call_github_api(f"branches/{branch_name}", headers, org, repo)
    return success


def _get_file_content(file_path: str, branch: str, headers: Dict[str, str], org: str, repo: str) -> Optional[str]:
    success, data = _call_github_api(f"contents/{file_path}?ref={branch}", headers, org, repo)
    if not success or not data:
        return None
    try:
        content_b64 = data.get("content", "").replace("\n", "")
        return base64.b64decode(content_b64).decode(CONFIG["GITHUB_API_CONFIG"]["file_encoding"], errors="replace")
    except Exception as e:
        print(f"[Êñá‰ª∂Ëß£Á†ÅÈîôËØØ] {file_path}: {e}", file=sys.stderr)
        return None


def _find_merged_pr(pr_title_keyword: str, base_branch: str, pr_state: str, headers: Dict[str, str], org: str, repo: str) -> Optional[Dict]:
    # Try first page(s) looking for matching merged PR
    page = 1
    per_page = CONFIG["GITHUB_API_CONFIG"]["api_per_page"]
    while True:
        endpoint = f"pulls"
        params = {"state": pr_state, "base": base_branch, "per_page": per_page, "page": page}
        success, pr_list = _call_github_api(f"{endpoint}", headers, org, repo, params=params)
        if not success or not pr_list:
            return None
        if not isinstance(pr_list, list):
            return None
        for pr in pr_list:
            title = pr.get("title", "")
            merged_at = pr.get("merged_at")
            if pr_title_keyword.lower() in title.lower() and merged_at:
                return pr
        # Stop if fewer than per_page results returned
        if len(pr_list) < per_page:
            break
        page += 1
    return None


def _verify_squash_merge(pr_number: int, headers: Dict[str, str], org: str, repo: str) -> Tuple[str, int]:
    """
    Return tuple: (method_str, parent_count)
    method_str: "Squash and Merge" | "OTHER" | "not found"
    parent_count: integer number of parents (0 if not found)
    """
    success, pr_detail = _call_github_api(f"pulls/{pr_number}", headers, org, repo)
    if not success or not pr_detail:
        return "not found", 0
    merge_commit_sha = pr_detail.get("merge_commit_sha")
    if not merge_commit_sha:
        return "not found", 0
    success, commit_detail = _call_github_api(f"commits/{merge_commit_sha}", headers, org, repo)
    if not success or not commit_detail:
        return "not found", 0
    parent_count = len(commit_detail.get("parents", []))
    commit_msg = commit_detail.get("commit", {}).get("message", "")
    expected_parent_count = CONFIG["GITHUB_API_CONFIG"]["merge_commit_parent_count"]
    if parent_count == expected_parent_count and f"#{pr_number}" in commit_msg:
        return "Squash and Merge", parent_count
    return "OTHER", parent_count


# --------------------------
# È™åËØÅÊ≠•È™§ÂÆûÁé∞
# --------------------------
def _verify_environment() -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['env_check']} È™åËØÅÁéØÂ¢ÉÈÖçÁΩÆ...")
    token, org = _load_env()
    if not token:
        print(f"[ÁéØÂ¢ÉÈîôËØØ] Êú™Ê£ÄÊµãÂà∞ÁéØÂ¢ÉÂèòÈáè {CONFIG['ENV_CONFIG']['github_token_var']}", file=sys.stderr)
        return None, None, None
    if not org:
        print(f"[ÁéØÂ¢ÉÈîôËØØ] Êú™Ê£ÄÊµãÂà∞ÁéØÂ¢ÉÂèòÈáè {CONFIG['ENV_CONFIG']['github_org_var']}", file=sys.stderr)
        return None, None, None
    headers = _build_headers(token)
    print(f"[ÁéØÂ¢ÉÂ∞±Áª™] org={org}, token=Â∑≤ÈÖçÁΩÆ")
    return token, org, headers


def _verify_branches(headers: Dict[str, str], org: str, repo: str) -> bool:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['branch_check']} È™åËØÅÂàÜÊîØÂ≠òÂú®ÊÄß...")
    b = CONFIG["TARGET_RESOURCE_CONFIG"]["branches"]
    if not _check_branch_exists(b["release_branch"], headers, org, repo):
        print(f"[ÂàÜÊîØÈîôËØØ] ÂèëÂ∏ÉÂàÜÊîØ {b['release_branch']} ‰∏çÂ≠òÂú®", file=sys.stderr)
        return False
    if not _check_branch_exists(b["base_branch"], headers, org, repo):
        print(f"[ÂàÜÊîØÈîôËØØ] Âü∫Á°ÄÂàÜÊîØ {b['base_branch']} ‰∏çÂ≠òÂú®", file=sys.stderr)
        return False
    print(f"[ÂàÜÊîØÈ™åËØÅÈÄöËøá] release={b['release_branch']}, base={b['base_branch']}")
    return True


def _verify_required_files(headers: Dict[str, str], org: str, repo: str) -> Tuple[bool, List[str]]:
    files = CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"]
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['file_check']} È™åËØÅÂÖ≥ÈîÆÊñá‰ª∂Ôºà{len(files)}Ôºâ...")
    ok = True
    failed_paths: List[str] = []
    for f in files:
        print(f"\n  È™åËØÅ {f['name']} -> {f['path']} @ {f['branch']}")
        # Ignore system files explicitly (though target files are named)
        if f["path"] in [".DS_Store", "Thumbs.db"]:
            print(f"  [ÂøΩÁï•] Á≥ªÁªüÊñá‰ª∂ {f['path']}")
            continue
        content = _get_file_content(f["path"], f["branch"], headers, org, repo)
        if not content:
            print(f"  [Êñá‰ª∂ÈîôËØØ] Êó†Ê≥ïËØªÂèñ {f['path']}", file=sys.stderr)
            ok = False
            failed_paths.append(f['path'])
            continue
        if len(content) < f["min_size"]:
            print(f"  [Êñá‰ª∂ÈîôËØØ] {f['path']} Â§ßÂ∞è {len(content)} < {f['min_size']}", file=sys.stderr)
            ok = False
            failed_paths.append(f['path'])
            continue
        # ÂÜÖÂÆπÊ†°È™åÔºàÂçï„ÄÅÂ§ö„ÄÅÂÖ≥ÈîÆËØçÔºâ
        if "required_content" in f:
            if f["required_content"] not in content:
                print(f"  [ÂÜÖÂÆπÈîôËØØ] Áº∫Â∞ë: {f['required_content'][:80]}...", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        if "required_contents" in f:
            missing = [c for c in f["required_contents"] if c not in content]
            if missing:
                print(f"  [ÂÜÖÂÆπÈîôËØØ] Áº∫Â§±Â§öÈ°π: {missing}", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        if "required_keywords" in f:
            missing = [k for k in f["required_keywords"] if k not in content]
            if missing:
                print(f"  [ÂÜÖÂÆπÈîôËØØ] Áº∫Â§±ÂÖ≥ÈîÆËØç: {missing}", file=sys.stderr)
                ok = False
                failed_paths.append(f['path'])
                continue
        print(f"  [Êñá‰ª∂È™åËØÅÈÄöËøá] {f['path']}")
    return ok, failed_paths


def _verify_release_pr(headers: Dict[str, str], org: str, repo: str) -> Optional[Dict]:
    print(f"\n{CONFIG['VERIFICATION_FLOW_CONFIG']['step_number_format']['pr_search']} Êü•ÊâæÂèëÂ∏É PR...")
    pr_cfg = CONFIG["PR_VERIFICATION_CONFIG"]
    base = CONFIG["TARGET_RESOURCE_CONFIG"]["branches"]["base_branch"]
    pr = _find_merged_pr(pr_cfg["pr_title_keyword"], base, pr_cfg["pr_state"], headers, org, repo)
    if not pr:
        print(f"[PR ÈîôËØØ] Êú™ÊâæÂà∞Á¨¶ÂêàÊù°‰ª∂ÁöÑÂ∑≤ÂêàÂπ∂ PRÔºàÂåÖÂê´ '{pr_cfg['pr_title_keyword']}'Ôºâ", file=sys.stderr)
        return None
    print(f"[PR ÊâæÂà∞] #{pr.get('number')} - {pr.get('title')}")
    return pr


# --------------------------
# Êä•ÂëäÁîüÊàê
# --------------------------
def _write_report(report_dir: str, payload_lines: List[str]) -> bool:
    # ensure dir exists
    os.makedirs(report_dir, exist_ok=True)
    report_file = os.path.join(report_dir, CONFIG["VERIFICATION_FLOW_CONFIG"]["report_file"])
    try:
        with open(report_file, "w", encoding="utf-8", newline="\n") as f:
            f.write("\n".join(payload_lines))
        print(f"[Êä•ÂëäÂ∑≤ÂÜôÂÖ•] {report_file}")
        return True
    except Exception as e:
        print(f"[Êä•ÂëäÂÜôÂÖ•ÈîôËØØ] {e}", file=sys.stderr)
        return False


def run_verification(cfg: Dict) -> bool:
    global CONFIG
    CONFIG = cfg
    sep = "=" * CONFIG["VERIFICATION_FLOW_CONFIG"]["separator_length"]
    print(sep)
    print("ÂºÄÂßãÊâßË°å GitHub ÂèëÂ∏ÉÊµÅÁ®ãÈ™åËØÅÔºàÁ§∫‰æãÔºâ")
    print(sep)

    _, org, headers = _verify_environment()
    if not org or not headers:
        # prepare fail report
        repo = CONFIG["TARGET_RESOURCE_CONFIG"]["target_repo"]
        lines = [
            "verification result: FAIL",
            f"repository: {org or 'UNKNOWN'}/{repo}",
            f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
            "release PR: not found",
            "merge method: not found  parents: 0",
            "files checked: 0/0  failed files: none",
            f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
        ]
        out_dir = CONFIG.get("OUTPUT_DIR", "")
        if out_dir:
            _write_report(out_dir, lines)
        return False

    repo = CONFIG["TARGET_RESOURCE_CONFIG"]["target_repo"]

    # Step 2: branches
    if not _verify_branches(headers, org, repo):
        result = "FAIL"
        pr_line = "release PR: not found"
        merge_line = "merge method: not found  parents: 0"
        files_checked_line = "files checked: 0/0  failed files: none"
        lines = [
            f"verification result: {result}",
            f"repository: {org}/{repo}",
            f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
            pr_line,
            merge_line,
            files_checked_line,
            f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
        ]
        _write_report(CONFIG.get("OUTPUT_DIR", ""), lines)
        return False

    # Step 3: files
    files_ok, failed_files = _verify_required_files(headers, org, repo)

    # Step 4: find PR
    pr = _verify_release_pr(headers, org, repo)
    pr_line = "release PR: not found"
    merge_line = "merge method: not found  parents: 0"
    pr_number = None
    pr_title = ""
    if pr:
        pr_number = pr.get("number")
        pr_title = pr.get("title", "").replace("\n", " ")
        pr_line = f"release PR: #{pr_number}  title: {pr_title}"
        # Step 5 & 6: merge target and method
        # Verify merge target
        success, pr_detail = _call_github_api(f"pulls/{pr_number}", headers, org, repo)
        if success and pr_detail:
            actual_base = pr_detail.get("base", {}).get("ref")
            expected_base = CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']
            # if target mismatch, set fail later
            if actual_base != expected_base:
                print(f"[ÂêàÂπ∂ÁõÆÊ†áÈîôËØØ] PR #{pr_number} ÂêàÂπ∂Âà∞„Äå{actual_base}„ÄçÔºåÈ¢ÑÊúü„Äå{expected_base}„Äç", file=sys.stderr)
        # verify merge method
        method_str, parents = _verify_squash_merge(pr_number, headers, org, repo)
        merge_line = f"merge method: {method_str}  parents: {parents}"
    else:
        # pr not found -> failure
        pass

    # decide overall PASS/FAIL
    passed_count = sum(1 for _ in CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"]) - len(failed_files)
    total_required = len(CONFIG["FILE_VERIFICATION_CONFIG"]["required_files"])
    files_checked_line = f"files checked: {total_required - len(failed_files)}/{total_required}  failed files: {','.join(failed_files) if failed_files else 'none'}"

    overall_ok = all([
        files_ok,
        pr is not None,
        (pr is None or (method_str == "Squash and Merge")),
    ])

    result = "PASS" if overall_ok else "FAIL"

    lines = [
        f"verification result: {result}",
        f"repository: {org}/{repo}",
        f"release branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['release_branch']}  base branch: {CONFIG['TARGET_RESOURCE_CONFIG']['branches']['base_branch']}",
        pr_line,
        merge_line,
        files_checked_line,
        f"timestamp: {datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}"
    ]

    out_dir = CONFIG.get("OUTPUT_DIR", "")
    if out_dir:
        _write_report(out_dir, lines)

    return overall_ok


if __name__ == "__main__":
    # ËøêË°å„ÄÇËã•Ë¶ÅÂ§ñÈÉ®Ëá™ÂÆö‰πâÈÖçÁΩÆÔºåÂèØÂú®Ê≠§Â§ÑÂä†ËΩΩ JSON Âπ∂‰º†ÂÖ• run_verification
    ok = run_verification(CONFIG)
    sys.exit(CONFIG["VERIFICATION_FLOW_CONFIG"]["exit_code"]["success"] if ok else CONFIG["VERIFICATION_FLOW_CONFIG"]["exit_code"]["failure"])
